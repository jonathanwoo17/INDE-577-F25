{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "778e23dd",
   "metadata": {},
   "source": [
    "# Principal Component Analysis (PCA) on the Breast Cancer Dataset\n",
    "\n",
    "This notebook demonstrates **Principal Component Analysis (PCA)** on the **Breast Cancer Wisconsin** dataset using your custom `pca` class (defined in `pca.py`).\n",
    "\n",
    "PCA is an **unsupervised** technique that finds new axes (principal components) that:\n",
    "- point in the directions of **maximum variance** in the data, and\n",
    "- are **orthogonal** (uncorrelated) with each other.\n",
    "\n",
    "Why do this?\n",
    "- **Dimensionality reduction:** compress 30 features into fewer components\n",
    "- **Visualization:** project high-dimensional data into 2D\n",
    "- **Noise reduction:** keep the strongest patterns, drop small-variance directions\n",
    "- **Preprocessing:** can improve speed/robustness for downstream models\n",
    "\n",
    "In this notebook we will:\n",
    "1. Fit PCA keeping **95% of the variance**\n",
    "2. Visualize variance explained (**scree** + **cumulative** plots)\n",
    "3. Visualize the data in **2D PCA space**\n",
    "4. Inspect **feature loadings** (what drives PC1/PC2)\n",
    "5. Quantify information loss using **reconstruction error**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8358dc0c",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "We use:\n",
    "- `load_breast_cancer` from scikit-learn to get a clean, well-known dataset\n",
    "- `matplotlib` for plots\n",
    "- your custom `pca` class for fitting, transforming, and inverse transforming\n",
    "\n",
    "**Note:** This notebook expects `pca.py` to be in the same directory as the notebook so that\n",
    "`from pca import pca` works.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d02b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "# Your custom PCA class (expects pca.py to be in the same folder)\n",
    "from pca import pca\n",
    "\n",
    "# Make plots look nicer\n",
    "plt.rcParams[\"figure.dpi\"] = 120"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e98f4c",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "\n",
    "The Breast Cancer Wisconsin dataset contains measurements computed from digitized images of breast mass cell nuclei.\n",
    "\n",
    "- **X** has shape `(n_samples, n_features)` and contains the numeric features\n",
    "- **y** has shape `(n_samples,)` and contains the class label:\n",
    "  - `0` = malignant\n",
    "  - `1` = benign\n",
    "\n",
    "Even though PCA is *unsupervised* (it does not use labels), we keep `y` so we can color points in plots\n",
    "and see whether the PCA projection separates the two classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a407c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "feature_names = data.feature_names\n",
    "target_names = data.target_names\n",
    "\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)\n",
    "print(\"Target names:\", target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1991852a",
   "metadata": {},
   "source": [
    "## Fit PCA (retain 95% variance)\n",
    "\n",
    "Here we choose `n_components=0.95`.\n",
    "\n",
    "In your implementation, passing a float means:\n",
    "> keep the **minimum number of components** such that the **cumulative explained variance** is at least that fraction.\n",
    "\n",
    "So `0.95` means:\n",
    "- PCA will pick **k** components (k is determined automatically)\n",
    "- those k components explain **≥ 95%** of the total variance in the dataset\n",
    "\n",
    "This is a practical way to reduce dimensionality without manually choosing k.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373c6ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pca(n_components=0.95)\n",
    "X_reduced = model.fit_transform(X)\n",
    "\n",
    "print(\"Reduced shape:\", X_reduced.shape)\n",
    "print(\"Number of components retained:\", model.n_components_)\n",
    "print(\"Total explained variance:\", model.explained_variance_ratio_.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a672de2",
   "metadata": {},
   "source": [
    "## Visualization: Explained Variance (Scree Plot)\n",
    "\n",
    "A **scree plot** shows the **explained variance ratio** for each principal component.\n",
    "\n",
    "Interpretation tips:\n",
    "- A very large bar for PC1 means most variation is captured by the first direction.\n",
    "- If the bars drop quickly then level off, the “elbow” region often suggests a good k.\n",
    "- Even if you keep many components, later components usually contribute only small amounts of variance.\n",
    "\n",
    "This plot helps answer: **“Which components matter most?”**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4655dc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "evr = model.explained_variance_ratio_\n",
    "k = np.arange(1, len(evr) + 1)\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.bar(k, evr)\n",
    "plt.xlabel(\"Principal Component\")\n",
    "plt.ylabel(\"Explained Variance Ratio\")\n",
    "plt.title(\"Scree Plot (Explained Variance by Component)\")\n",
    "plt.xticks(k)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016175c4",
   "metadata": {},
   "source": [
    "## Visualization: Cumulative Explained Variance\n",
    "\n",
    "This plot shows the **running total** of explained variance as we add more components.\n",
    "\n",
    "Interpretation tips:\n",
    "- Pick a target threshold like 90%, 95%, or 99%\n",
    "- Read off the number of components needed to reach that threshold\n",
    "- Diminishing returns: each extra component adds less and less explained variance\n",
    "\n",
    "This plot helps answer: **“How many components do I need?”**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c81e685",
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_evr = np.cumsum(evr)\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.plot(k, cum_evr, marker=\"o\")\n",
    "plt.axhline(0.90, linestyle=\"--\")\n",
    "plt.axhline(0.95, linestyle=\"--\")\n",
    "plt.axhline(0.99, linestyle=\"--\")\n",
    "plt.xlabel(\"Number of Components\")\n",
    "plt.ylabel(\"Cumulative Explained Variance\")\n",
    "plt.title(\"Cumulative Explained Variance\")\n",
    "plt.xticks(k)\n",
    "plt.ylim(0, 1.01)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd10aff",
   "metadata": {},
   "source": [
    "## Visualization: 2D PCA Projection (Colored by Class)\n",
    "\n",
    "To visualize the data, we fit a *separate* PCA model with `n_components=2`.\n",
    "\n",
    "Why a separate model?\n",
    "- The 95% variance model may keep more than 2 components (k could be 8–12-ish)\n",
    "- For plotting, we need exactly 2 columns (PC1 and PC2)\n",
    "\n",
    "Even though PCA does not use labels, coloring by `y` helps us see:\n",
    "- whether the dominant variance directions align with class separation, and\n",
    "- how much overlap exists between malignant and benign samples in 2D.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70facbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_2d = pca(n_components=2)\n",
    "X_2d = pca_2d.fit_transform(X)\n",
    "\n",
    "print(\"2D shape:\", X_2d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a5f677",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 5))\n",
    "sc = plt.scatter(\n",
    "    X_2d[:, 0],\n",
    "    X_2d[:, 1],\n",
    "    c=y,\n",
    "    alpha=0.75,\n",
    ")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.title(\"Breast Cancer Dataset in 2D PCA Space\")\n",
    "\n",
    "cbar = plt.colorbar(sc)\n",
    "cbar.set_ticks([0, 1])\n",
    "cbar.set_ticklabels([f\"0 = {target_names[0]}\", f\"1 = {target_names[1]}\"])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67926c75",
   "metadata": {},
   "source": [
    "## Visualization: Class Centroids in PCA Space\n",
    "\n",
    "A **centroid** is the mean position of a group of points.\n",
    "\n",
    "Here we compute the centroid of each class in the 2D PCA space:\n",
    "- one centroid for malignant samples\n",
    "- one centroid for benign samples\n",
    "\n",
    "If the centroids are far apart, it suggests that the PCA projection separates classes well.\n",
    "If they overlap, the separation is weaker (at least along PC1/PC2).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d55076",
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids = np.vstack([X_2d[y == cls].mean(axis=0) for cls in [0, 1]])\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.scatter(X_2d[:, 0], X_2d[:, 1], c=y, alpha=0.25)\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1], s=200, marker=\"X\")\n",
    "for i, name in enumerate(target_names):\n",
    "    plt.text(centroids[i, 0], centroids[i, 1], f\"  {name}\", va=\"center\")\n",
    "\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.title(\"2D PCA Space with Class Centroids\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8baf2b12",
   "metadata": {},
   "source": [
    "## Feature Loadings for PC1 and PC2\n",
    "\n",
    "Each principal component is a weighted combination of the original features.\n",
    "\n",
    "Those weights are called **loadings**:\n",
    "- A large **positive** loading means that feature increases as the component increases.\n",
    "- A large **negative** loading means that feature decreases as the component increases.\n",
    "- The **absolute value** indicates strength/importance for that component.\n",
    "\n",
    "We display the **top 10 features by absolute loading** for PC1 and PC2.\n",
    "This helps interpret what the axes “mean” in terms of original measurements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ebc7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "components = pca_2d.components_  # shape (2, n_features)\n",
    "\n",
    "def top_loadings(component_vector, feature_names, top_n=10):\n",
    "    idx = np.argsort(np.abs(component_vector))[::-1][:top_n]\n",
    "    return idx, component_vector[idx]\n",
    "\n",
    "for i in range(2):\n",
    "    idx, vals = top_loadings(components[i], feature_names, top_n=10)\n",
    "    plt.figure(figsize=(7, 4))\n",
    "    plt.barh(np.arange(len(idx))[::-1], vals)\n",
    "    plt.yticks(np.arange(len(idx))[::-1], feature_names[idx])\n",
    "    plt.xlabel(\"Loading (signed)\")\n",
    "    plt.title(f\"Top 10 Feature Loadings for PC{i+1}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3e56bd",
   "metadata": {},
   "source": [
    "## Reconstruction and Information Loss\n",
    "\n",
    "PCA can be viewed as a compression method:\n",
    "1. **Transform:** map X → Z (lower-dimensional representation)\n",
    "2. **Inverse transform:** map Z → X̂ (reconstruction in original feature space)\n",
    "\n",
    "The difference between `X` and `X_hat` is what PCA cannot represent using the chosen number of components.\n",
    "\n",
    "We quantify that loss with **mean squared error (MSE)**:\n",
    "- smaller MSE = better reconstruction (less information lost)\n",
    "- larger MSE = more information lost (more aggressive compression)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9e852d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_hat = model.inverse_transform(X_reduced)\n",
    "reconstruction_mse = np.mean((X - X_hat) ** 2)\n",
    "print(\"Reconstruction MSE (using 95% variance components):\", reconstruction_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d35947",
   "metadata": {},
   "source": [
    "## Visualization: Reconstruction Error vs Number of Components\n",
    "\n",
    "This experiment makes the compression tradeoff very clear.\n",
    "\n",
    "For k = 1, 2, ..., max_k:\n",
    "- fit PCA with exactly k components\n",
    "- reconstruct the data\n",
    "- compute reconstruction MSE\n",
    "\n",
    "You should see a downward trend:\n",
    "- small k → high error (too much information discarded)\n",
    "- larger k → lower error (more information preserved)\n",
    "\n",
    "This plot helps answer: **“How much error do I introduce by choosing k?”**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa87f6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_k = min(15, X.shape[1])  # keep this small so it runs fast\n",
    "mses = []\n",
    "ks = np.arange(1, max_k + 1)\n",
    "\n",
    "for k in ks:\n",
    "    m = pca(n_components=int(k))\n",
    "    Z = m.fit_transform(X)\n",
    "    X_rec = m.inverse_transform(Z)\n",
    "    mses.append(np.mean((X - X_rec) ** 2))\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.plot(ks, mses, marker=\"o\")\n",
    "plt.xlabel(\"Number of Components (k)\")\n",
    "plt.ylabel(\"Reconstruction MSE\")\n",
    "plt.title(\"Reconstruction Error vs Number of Components\")\n",
    "plt.xticks(ks)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c08b15d",
   "metadata": {},
   "source": [
    "## PCA Score\n",
    "\n",
    "Your `score` method returns:\n",
    "\n",
    "> **negative mean squared reconstruction error**\n",
    "\n",
    "So:\n",
    "- higher score (less negative) = better reconstruction\n",
    "- lower score (more negative) = worse reconstruction\n",
    "\n",
    "This is a convenient single-number summary of reconstruction quality for a given dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12916260",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64708ec",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "What we learned from PCA on this dataset:\n",
    "\n",
    "- The Breast Cancer dataset has **30 features**, but PCA can compress it to fewer dimensions while retaining most variance.\n",
    "- The scree and cumulative variance plots help choose the number of components in a principled way.\n",
    "- A **2D PCA projection** can already show meaningful structure and partial class separation.\n",
    "- **Loadings** reveal which original features drive the main directions of variation.\n",
    "- **Reconstruction MSE** makes the compression vs. accuracy tradeoff measurable.\n",
    "\n",
    "Next ideas:\n",
    "- Try different variance thresholds (0.90, 0.99)\n",
    "- Standardize vs. no standardization (`scale=True/False`)\n",
    "- Use PCA features as input to a classifier and compare accuracy vs. original features\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
